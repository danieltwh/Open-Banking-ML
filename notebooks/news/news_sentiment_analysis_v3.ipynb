{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_comp</th>\n",
       "      <th>vader_label</th>\n",
       "      <th>label</th>\n",
       "      <th>include</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Column1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Key remarks from Russian c.bank governor after...</td>\n",
       "      <td>2022-02-11T12:26:00Z</td>\n",
       "      <td>https://www.reuters.com/markets/currencies/key...</td>\n",
       "      <td>MOSCOW, Feb 11 (Reuters) - The Russian Central...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIVE MARKETS Forex: worrying about an inverted...</td>\n",
       "      <td>2022-02-18T12:33:00Z</td>\n",
       "      <td>https://www.reuters.com/markets/stocks/live-ma...</td>\n",
       "      <td>Feb 18 - Welcome to the home for real-time cov...</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dollar takes a break from this week's slide ah...</td>\n",
       "      <td>2022-02-03T02:18:00Z</td>\n",
       "      <td>https://www.reuters.com/markets/europe/dollar-...</td>\n",
       "      <td>SINGAPORE, Feb 3 (Reuters) - The dollar found ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euro bounce pauses ahead of U.S. inflation - R...</td>\n",
       "      <td>2022-02-08T01:16:00Z</td>\n",
       "      <td>https://www.reuters.com/business/euro-bounce-p...</td>\n",
       "      <td>SINGAPORE, Feb 8 (Reuters) - A resurgent euro ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dollar near two-week high amid jitters over ha...</td>\n",
       "      <td>2022-01-25T01:52:00Z</td>\n",
       "      <td>https://www.reuters.com/markets/europe/dollar-...</td>\n",
       "      <td>TOKYO, Jan 25 (Reuters) - The safe-haven U.S. ...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "Column1                                                      \n",
       "1        Key remarks from Russian c.bank governor after...   \n",
       "2        LIVE MARKETS Forex: worrying about an inverted...   \n",
       "3        Dollar takes a break from this week's slide ah...   \n",
       "4        Euro bounce pauses ahead of U.S. inflation - R...   \n",
       "5        Dollar near two-week high amid jitters over ha...   \n",
       "\n",
       "                  publishedAt  \\\n",
       "Column1                         \n",
       "1        2022-02-11T12:26:00Z   \n",
       "2        2022-02-18T12:33:00Z   \n",
       "3        2022-02-03T02:18:00Z   \n",
       "4        2022-02-08T01:16:00Z   \n",
       "5        2022-01-25T01:52:00Z   \n",
       "\n",
       "                                                       url  \\\n",
       "Column1                                                      \n",
       "1        https://www.reuters.com/markets/currencies/key...   \n",
       "2        https://www.reuters.com/markets/stocks/live-ma...   \n",
       "3        https://www.reuters.com/markets/europe/dollar-...   \n",
       "4        https://www.reuters.com/business/euro-bounce-p...   \n",
       "5        https://www.reuters.com/markets/europe/dollar-...   \n",
       "\n",
       "                                                   content  vader_neg  \\\n",
       "Column1                                                                 \n",
       "1        MOSCOW, Feb 11 (Reuters) - The Russian Central...      0.000   \n",
       "2        Feb 18 - Welcome to the home for real-time cov...      0.211   \n",
       "3        SINGAPORE, Feb 3 (Reuters) - The dollar found ...      0.000   \n",
       "4        SINGAPORE, Feb 8 (Reuters) - A resurgent euro ...      0.000   \n",
       "5        TOKYO, Jan 25 (Reuters) - The safe-haven U.S. ...      0.197   \n",
       "\n",
       "         vader_neu  vader_pos  vader_comp  vader_label  label  include  \\\n",
       "Column1                                                                  \n",
       "1            1.000        0.0      0.0000            0      0        1   \n",
       "2            0.789        0.0     -0.3400           -1     -1        1   \n",
       "3            1.000        0.0      0.0000            0     -1        1   \n",
       "4            1.000        0.0      0.0000            0      1        1   \n",
       "5            0.803        0.0     -0.4019           -1     -1        1   \n",
       "\n",
       "         relevant  \n",
       "Column1            \n",
       "1               0  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "5               1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import tensorflow as tf\n",
    "df = pd.read_csv(\"../../data/raw/news/newsapiorg_labelled.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1049\n",
       "-1     569\n",
       " 1     535\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1049\n",
       "1     569\n",
       "0     535\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"label\"] == 0, 'label'] = 2\n",
    "df.loc[df['label'] == 1, 'label'] = 0\n",
    "df.loc[df['label'] == -1, 'label'] = 1\n",
    "df.label.value_counts() \n",
    "#0: Positive\n",
    "#1: Negative\n",
    "#2: Neutral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(df.title, df.label, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_ds_raw = Dataset.from_dict({'text': train_inputs, 'labels': train_labels})\n",
    "valid_ds_raw = Dataset.from_dict({'text': valid_inputs, 'labels': valid_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 1722\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 431\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 18.04ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 83.32ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "train_ds = train_ds_raw.map(tokenize, batched=True)\n",
    "valid_ds = valid_ds_raw.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1722\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 431\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "MODEL_PATH = \"../../results/models/FinBERT_v1.0\"\n",
    "train_args = TrainingArguments(\n",
    "    MODEL_PATH,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    evaluation_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluation(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\wangt\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1722\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 540\n",
      " 20%|██        | 108/540 [10:45<40:55,  5.68s/it] The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 431\n",
      "  Batch size = 16\n",
      "                                                 \n",
      " 20%|██        | 108/540 [11:48<40:55,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5825806260108948, 'eval_accuracy': 0.7633410672853829, 'eval_runtime': 63.4456, 'eval_samples_per_second': 6.793, 'eval_steps_per_second': 0.426, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 216/540 [21:38<30:03,  5.57s/it]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 431\n",
      "  Batch size = 16\n",
      "                                                 \n",
      " 40%|████      | 216/540 [22:33<30:03,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5182616710662842, 'eval_accuracy': 0.814385150812065, 'eval_runtime': 54.8006, 'eval_samples_per_second': 7.865, 'eval_steps_per_second': 0.493, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 324/540 [33:41<17:45,  4.93s/it]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 431\n",
      "  Batch size = 16\n",
      "                                                 \n",
      " 60%|██████    | 324/540 [34:38<17:45,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5816057920455933, 'eval_accuracy': 0.8167053364269141, 'eval_runtime': 56.3756, 'eval_samples_per_second': 7.645, 'eval_steps_per_second': 0.479, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 432/540 [45:15<08:26,  4.69s/it]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 431\n",
      "  Batch size = 16\n",
      "                                                 \n",
      " 80%|████████  | 432/540 [46:12<08:26,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6071388125419617, 'eval_accuracy': 0.8259860788863109, 'eval_runtime': 56.5632, 'eval_samples_per_second': 7.62, 'eval_steps_per_second': 0.477, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 500/540 [52:21<03:51,  5.78s/it]Saving model checkpoint to ../../results/models/FinBERT_v1.0\\checkpoint-500\n",
      "Configuration saved in ../../results/models/FinBERT_v1.0\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3865, 'learning_rate': 1.646090534979424e-06, 'epoch': 4.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../../results/models/FinBERT_v1.0\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ../../results/models/FinBERT_v1.0\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ../../results/models/FinBERT_v1.0\\checkpoint-500\\special_tokens_map.json\n",
      "100%|██████████| 540/540 [55:49<00:00,  4.56s/it]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 431\n",
      "  Batch size = 16\n",
      "                                                 \n",
      "100%|██████████| 540/540 [56:29<00:00,  4.56s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 540/540 [56:29<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6239088773727417, 'eval_accuracy': 0.8329466357308585, 'eval_runtime': 40.6166, 'eval_samples_per_second': 10.611, 'eval_steps_per_second': 0.665, 'epoch': 5.0}\n",
      "{'train_runtime': 3389.8008, 'train_samples_per_second': 2.54, 'train_steps_per_second': 0.159, 'train_loss': 0.3734118797160961, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=540, training_loss=0.3734118797160961, metrics={'train_runtime': 3389.8008, 'train_samples_per_second': 2.54, 'train_steps_per_second': 0.159, 'train_loss': 0.3734118797160961, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../results/models/FinBERT_v1.0\n",
      "Configuration saved in ../../results/models/FinBERT_v1.0\\config.json\n",
      "Model weights saved in ../../results/models/FinBERT_v1.0\\pytorch_model.bin\n",
      "tokenizer config file saved in ../../results/models/FinBERT_v1.0\\tokenizer_config.json\n",
      "Special tokens file saved in ../../results/models/FinBERT_v1.0\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../results/models/FinBERT_v1.0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../results/models/FinBERT_v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../../results/models/FinBERT_v1.0\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../../results/models/FinBERT_v1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trained_bert = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [00:00<00:00, 3918.19ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 431\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = valid_ds_raw.map(tokenize, batched=False)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 431\n",
      "  Batch size = 8\n",
      "100%|██████████| 54/54 [00:31<00:00,  1.55it/s]"
     ]
    }
   ],
   "source": [
    "trained_model = Trainer(\n",
    "    trained_bert,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "output = trained_model.predict(\n",
    "    test_dataset=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.9047167 ,  3.4954553 , -1.1367399 ],\n",
       "       [-1.5548592 , -1.685975  ,  3.67654   ],\n",
       "       [-1.8579643 ,  3.4466624 , -1.3580679 ],\n",
       "       ...,\n",
       "       [-1.6760224 , -0.4588015 ,  2.7747884 ],\n",
       "       [-2.2575953 ,  3.179277  ,  0.30187687],\n",
       "       [-1.6353482 , -1.3618524 ,  3.5516517 ]], dtype=float32), label_ids=array([1, 2, 1, 0, 2, 0, 0, 0, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 1, 2, 2, 1,\n",
       "       2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 0, 1, 1, 1, 2, 2, 1, 0, 2, 1, 2, 2,\n",
       "       2, 2, 1, 2, 0, 0, 2, 1, 2, 2, 0, 0, 0, 2, 2, 0, 1, 2, 2, 2, 0, 2,\n",
       "       0, 0, 2, 0, 0, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n",
       "       2, 2, 0, 2, 2, 1, 2, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 1, 2, 2, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 1, 0, 2, 2, 0,\n",
       "       1, 0, 2, 1, 1, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 1, 2, 2, 1, 0, 0,\n",
       "       2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 1, 0, 2, 0, 1, 1, 1, 1, 2, 2, 2, 0,\n",
       "       0, 2, 0, 1, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 0, 0, 1, 2, 2,\n",
       "       2, 2, 1, 1, 2, 1, 2, 2, 0, 2, 2, 1, 0, 2, 0, 0, 1, 0, 0, 1, 1, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 1, 0, 1, 2, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2,\n",
       "       1, 2, 1, 0, 1, 2, 1, 1, 1, 2, 1, 2, 2, 0, 1, 1, 0, 1, 2, 0, 2, 2,\n",
       "       0, 1, 2, 1, 2, 0, 2, 0, 2, 2, 1, 2, 0, 1, 1, 2, 0, 2, 0, 2, 1, 2,\n",
       "       1, 2, 2, 1, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       1, 1, 1, 2, 2, 1, 0, 2, 2, 0, 1, 1, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 1, 0, 0, 0, 2, 0,\n",
       "       1, 1, 2, 1, 0, 2, 1, 2, 2, 2, 0, 2, 2, 2, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       2, 1, 1, 2, 0, 1, 2, 0, 2, 1, 2, 0, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2,\n",
       "       1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 1, 2], dtype=int64), metrics={'test_loss': 0.6239088773727417, 'test_runtime': 32.1071, 'test_samples_per_second': 13.424, 'test_steps_per_second': 1.682})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])\n",
    "encoder.inverse_transform([np.argmax(i) for i in output.predictions])\n",
    "preds = [np.argmax(i) for i in output.predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8329466357308585"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_labels, preds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ce811e1c0670d5f467c01af822c606772b3a1d3f5126339046f7664fa0ae9ff"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
